# -*- coding: utf-8 -*-
"""mtnrwanda-dag.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lrx6aQ6v1m_UI-dq-6G0TtqyKglT3HSa
"""

import pandas as pd
import pymongo
from pymongo import MongoClient
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import logging


def extract_data():
    """Extracts data from CSV files"""
    try:
      customer_data_df = pd.read_csv('customer_data.csv')
      order_data_df = pd.read_csv('order_data.csv')
      payment_data_df = pd.read_csv('payment_data.csv')
      # return dataframes as a dictionary
      return {'customer_data': customer_data_df, 'order_data': order_data_df, 'payment_data': payment_data_df}
    except Exception as e:
        # Log error message
        logging.error("Error during data extraction: {}".format(str(e)))
        raise


def transform_data(**context):
  """Transforms data and calculates customer lifetime value"""
  try:
    # retrieve dataframes from context
    customer_df = context['task_instance'].xcom_pull(task_ids='extract_data')['customer_data']
    order_df = context['task_instance'].xcom_pull(task_ids='extract_data')['order_data']
    payment_df = context['task_instance'].xcom_pull(task_ids='extract_data')['payment_data']
    
      # Convert date fields to the correct format using pd.to_datetime
    customer_df['date_of_birth'] = pd.to_datetime(customer_df['date_of_birth'])
    order_df['order_date'] = pd.to_datetime(order_df['order_date'])
    payment_df['payment_date'] = pd.to_datetime(payment_df['payment_date'])
    
    # Merge customer and order dataframes on the customer_id column
    merged_df = pd.merge(customer_df, order_df, on='customer_id')
    
    # Merge payment dataframe with the merged dataframe on the order_id and customer_id columns
    final_df = pd.merge(merged_df, payment_df, on=['order_id', 'customer_id'])
    
    # Drop unnecessary columns like customer_id and order_id
    final_df.drop(['customer_id', 'order_id'], axis=1, inplace=True)
    
    # Group the data by customer and aggregate the amount paid using sum
    agg_df = final_df.groupby('customer_name').agg({'amount_paid': 'sum'})
    
    # Create a new column to calculate the total value of orders made by each customer
    agg_df['total_order_value'] = final_df.groupby('customer_name').agg({'order_value': 'sum'})
    
    # Calculate the customer lifetime value using the formula CLV = (average order value x purchase frequency) x customer lifespan
    agg_df['lifespan'] = (max(final_df['payment_date']) - min(final_df['payment_date'])).days / 365
    agg_df['purchase_frequency'] = final_df.groupby('customer_name').size() / len(final_df['payment_date'].unique())
    agg_df['avg_order_value'] = agg_df['total_order_value'] / agg_df['purchase_frequency']
    agg_df['clv'] = (agg_df['avg_order_value'] * agg_df['purchase_frequency']) * agg_df['lifespan']
    
    return agg_df
  except Exception as e:
        # Log error message
        logging.error("Error during data transformation: {}".format(str(e)))
        raise


def load_data(*args):
    """Loads transformed data into MongoDB"""
    try:
      client = MongoClient("mongodb+srv://{username}:{password}@{cluster-name}.{region}.mongodb.net/{db-name}?retryWrites=true&w=majority"
      .format(username=MONGO_USERNAME,password=MONGO_PASSWORD,
              cluster_name=MONGO_CLUSTER_NAME,region=MONGO_REGION,
              db_name=MONGO_DB_NAME), ssl=True, authSource="admin", authMechanism='SCRAM-SHA-256')
      db = client[MONGO_DB_NAME]
      # Convert transformed data to dictionary
      transformed_data_dict = transformed_data.to_dict('records')
      # Insert transformed data into MongoDB
      result = db[mongo_collection_name].insert_many(transformed_data_dict)
      # Log success message
      logging.info(f"Data has been loaded into MongoDB. Inserted {len(result.inserted_ids)} records.")
    except Exception as e:
      # Log error message
      logging.error(f"An error occurred while loading data into MongoDB. Error: {str(e)}")
    finally:
      # Close MongoDB connection
      client.close()
    
    
# define DAG parameters
default_args = {
    'owner': 'MTN',
    'depends_on_past': False,
    'start_date': datetime(2022, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# define DAG
dag = DAG(
    'mtn_data_pipeline',
    default_args=default_args,
    description='MTN data pipeline to extract, transform, and load data from multiple CSV files to MongoDB',
    schedule_interval=timedelta(days=1),
)

# define tasks
extract_data_task = PythonOperator(
task_id='extract_data',
python_callable=extract_data,
dag=dag,
)

transform_data_task = PythonOperator(
task_id='transform_data',
python_callable=transform_data,
dag=dag,
)

load_data_task = PythonOperator(
task_id='load_data',
python_callable=load_data,
op_kwargs={'mongo_uri': MONGO_URI, 'mongo_db': MONGO_DB_NAME, 'mongo_collection_name': MONGO_COLLECTION_NAME},
dag=dag,
)

# Define task dependencies
transform_data_task.set_upstream(extract_data_task)
load_data_task.set_upstream(transform_data_task)

# Define SLA
sla = timedelta(hours=1)

# Define the SLA email notification
email_notification = EmailOperator(
task_id='sla_email_notification',
to=EMAIL_RECIPIENTS,
subject='Data pipeline SLA violation',
html_content=f"""The data pipeline has not been completed within the expected timeframe of {sla}.
Please investigate and take appropriate action.""",
dag=dag
)

# Define SLA
sla = timedelta(hours=1)

# Define the SLA email notification
email_notification = EmailOperator(
task_id='sla_email_notification',
to=EMAIL_RECIPIENTS,
subject='Data pipeline SLA violation',
html_content=f"""The data pipeline has not been completed within the expected timeframe of {sla}.
Please investigate and take appropriate action.""",
dag=dag
)

# Define the SLA monitoring task
sla_monitoring = SlaMissCallbackOperator(
task_id='sla_miss_callback',
sla=sla,
callback=email_notification.execute,
dag=dag
)

# Set the dependency for the SLA monitoring task
sla_monitoring.set_upstream(load_data_task)